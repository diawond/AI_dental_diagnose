# -*- coding: utf-8 -*-
"""AI_dental_diagnose.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wdbnbKQI5uymAfiwT3KpZlD6K8PnQrLr
"""

!pip install gradio openai sentence-transformers inference-sdk

!pip install openai==0.28

import pandas as pd
from sentence_transformers import SentenceTransformer, util

# ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ
rag_df = pd.read_csv("/content/dental_training_data.csv")  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô path ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
assert all(col in rag_df.columns for col in ["Question", "Category", "SubCategory"]), "Missing required columns"

# ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° embedding model
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
rag_embeddings = embedding_model.encode(rag_df["Question"].tolist(), convert_to_tensor=True)

def retrieve_similar_context(user_symptom: str, top_k: int = 3) -> str:
    user_embedding = embedding_model.encode(user_symptom, convert_to_tensor=True)
    hits = util.semantic_search(user_embedding, rag_embeddings, top_k=top_k)[0]
    results = []
    for hit in hits:
        idx = hit['corpus_id']
        q = rag_df.iloc[idx]["Question"]
        c = rag_df.iloc[idx]["Category"]
        s = rag_df.iloc[idx]["SubCategory"]
        results.append(f'{{"question": "{q}", "category": "{c}", "subcategory": "{s}"}}')
    return "\n".join(results)

import openai
import os
import json

# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenRouter (‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÉ‡∏™‡πà key)
openai.api_key = ""  # ‡πÉ‡∏™‡πàkey api
openai.api_base = "https://openrouter.ai/api/v1"

def generate_llm_prompt(description: str, image_analysis_results: list = None) -> str:
    context = retrieve_similar_context(description, top_k=3)

    image_info = ""
    if image_analysis_results:
        image_info = "\n**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û (‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î):**\n" # ‡πÄ‡∏ô‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
        for prediction in image_analysis_results:
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
            image_info += f'- **Class:** {prediction.get("class", "N/A")}, **Confidence:** {prediction.get("confidence", "N/A"):.2f}, **Class ID:** {prediction.get("class_id", "N/A")}\n'


    return f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏±‡∏ô‡∏ï‡πÅ‡∏û‡∏ó‡∏¢‡πå AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ï‡∏Å‡∏£‡∏£‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢:**
"{description}"

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏à‡∏£‡∏¥‡∏á:**
{context}
{image_info}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô

‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
{{
  "predicted_category": "...",
  "predicted_subcategory": "...",
  "reasoning": "...",
}}
"""

def analyze_text_with_llm(prompt: str) -> dict:
    try:
        response = openai.ChatCompletion.create(
          model="openai/gpt-5-chat",
            messages=[
                {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏±‡∏ô‡∏ï‡πÅ‡∏û‡∏ó‡∏¢‡πå AI"},
                {"role": "user", "content": prompt}
            ]
        )
        print(response)
        content = response["choices"][0]["message"]["content"]

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ content ‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô parse
        try:
            return json.loads(content.strip())
        except json.JSONDecodeError:
            print("Warning: LLM response was not valid JSON.")
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ default ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            return {
                "predicted_category": "Unknown",
                "predicted_subcategory": "Unknown",
                "reasoning": "LLM did not return valid JSON. Could not analyze text."
            }

    except Exception as e:
        print("LLM Error:", e)
        return {}

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
# image_results_example = [{"class": "CALCULUS", "confidence": 0.95, "class_id": 0}]
# prompt = generate_llm_prompt("‡∏ü‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏™‡∏ß‡∏¢ ‡∏°‡∏µ‡∏´‡∏¥‡∏ô‡∏õ‡∏π‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏à‡πá‡∏ö‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏Å‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏á‡∏∑‡∏≠‡∏Å‡πÅ‡∏î‡∏á‡∏ö‡∏ß‡∏°‡∏£‡πà‡∏ß‡∏°‡∏î‡πâ‡∏ß‡∏¢", image_results_example)
# analyze_text_with_llm(prompt)

from inference_sdk import InferenceHTTPClient

roboflow_client = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key=""  # ‡πÉ‡∏™‡πà api key
)

def analyze_image_with_roboflow(image_path: str):
    result = roboflow_client.infer(image_path, model_id="dent_final2-hqfas/3")
    return result.get("predictions", [])

def merge_results(text_result, image_result):
    image_classes = [p["class"] for p in image_result]
    max_conf = max([p["confidence"] for p in image_result], default=0.0)

    category = text_result.get("predicted_category", "")
    subcat = text_result.get("predicted_subcategory", "")
    reasoning = text_result.get("reasoning", "")

    return {
        "category": category,
        "subcategory": subcat,
        "reasoning": reasoning,
        "image_findings": image_result,
        "image_max_confidence": max_conf
    }

import gradio as gr
import uuid
import cv2 # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ OpenCV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û
import numpy as np

def process_dental_assistant(symptoms, image):
    llm_input_description = ""
    image_result = []
    output_image_path = None

    # üñºÔ∏è ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ Roboflow ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏†‡∏≤‡∏û
    if image is not None:
        image_path = image
        image_result = analyze_image_with_roboflow(image_path)
        # üé® ‡∏ß‡∏≤‡∏î bounding boxes ‡∏ö‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        img = cv2.imread(image_path)
        if img is not None:
            for prediction in image_result:
                x = int(prediction['x'])
                y = int(prediction['y'])
                width = int(prediction['width'])
                height = int(prediction['height'])
                class_name = prediction['class']
                confidence = prediction['confidence']

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì coordinates ‡∏Ç‡∏≠‡∏á bounding box (OpenCV ‡πÉ‡∏ä‡πâ top-left corner ‡πÅ‡∏•‡∏∞ bottom-right corner)
                x1 = int(x - width/2)
                y1 = int(y - height/2)
                x2 = int(x + width/2)
                y2 = int(y + height/2)

                # ‡∏ß‡∏≤‡∏î‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°
                color = (0, 255, 0) # ‡∏™‡∏µ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß (BGR)
                thickness = 2
                cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)

                # ‡πÉ‡∏™‡πà label
                label = f"{class_name}: {confidence:.2f}"
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.5
                font_thickness = 1
                text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]
                text_x = x1
                text_y = y1 - 10 if y1 - 10 > 10 else y1 + text_size[1] + 10 # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á label

                cv2.putText(img, label, (text_x, text_y), font, font_scale, color, font_thickness, cv2.LINE_AA)

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
            output_image_path = f"/tmp/{uuid.uuid4()}.png"
            cv2.imwrite(output_image_path, img)
        else:
            print(f"Error loading image: {image_path}")


    # ‚û°Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
    if symptoms and symptoms.strip(): # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ input symptoms ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ
        llm_input_description = symptoms
        print(f"‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {llm_input_description}")
    elif image_result: # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ input symptoms ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û
        image_classes = [p["class"] for p in image_result]
        llm_input_description = "‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: " + ", ".join(image_classes)
        print(f"‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û: {llm_input_description}")
    else: # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û ‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        llm_input_description = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡∏†‡∏≤‡∏û"
        print(f"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï")
        return {"category": "N/A", "subcategory": "N/A", "reasoning": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï", "image_findings": [], "image_max_confidence": 0.0}, None


    # üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ LLM (‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û)
    prompt = generate_llm_prompt(llm_input_description, image_result) # ‡∏™‡πà‡∏á image_result ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
    text_result = analyze_text_with_llm(prompt)
    print(text_result)

    # ‚Ü§Ô∏è ‡∏£‡∏ß‡∏°‡∏ú‡∏•
    merged = merge_results(text_result, image_result)

    # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö output ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û
    if image is None:
        return merged.get("reasoning", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û"), None # ‡∏™‡πà‡∏á‡πÅ‡∏Ñ‡πà reasoning ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    else:
        # ‚û°Ô∏è ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå LLM ‡πÅ‡∏•‡∏∞ path ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏î
        return merged, output_image_path

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á GUI
gr.Interface(
    fn=process_dental_assistant,
    inputs=[
        gr.Textbox(label="‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏û‡∏ö (‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πá‡πÑ‡∏î‡πâ)", lines=4, elem_id="symptoms-input"), # ‡πÄ‡∏û‡∏¥‡πà‡∏° elem_id
        gr.Image(type="filepath", label="‡πÅ‡∏ô‡∏ö‡∏†‡∏≤‡∏û‡∏ü‡∏±‡∏ô (JPG/PNG)") # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ image ‡πÑ‡∏°‡πà require
    ],
    outputs=[
        gr.JSON(label="‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏î‡∏¢ AI (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)", elem_id="analysis-output"), # ‡πÄ‡∏û‡∏¥‡πà‡∏° elem_id
        gr.Image(label="‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏î‡∏¢ AI (‡∏†‡∏≤‡∏û)") # ‡πÄ‡∏û‡∏¥‡πà‡∏° output ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏î
    ],
    title="ü¶∑ AI Dental Assistant with LLM + Roboflow + RAG", # ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠ title
    css="""
    #symptoms-input textarea {
        font-size: 1.2em !important; /* Adjust the size as needed */
    }
    #analysis-output .json-viewer {
        font-size: 1.2em !important; /* Adjust the size as needed */
    }
    """ # ‡πÄ‡∏û‡∏¥‡πà‡∏° CSS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏Ç‡∏≠‡∏á input ‡πÅ‡∏•‡∏∞ output json
).launch(share=True)